{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f166bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import itertools\n",
    "import textdistance\n",
    "import numpy as np\n",
    "import random as rd\n",
    "from random import seed\n",
    "from random import randint\n",
    "from random import choices\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import math\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952b567e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data and get every tv as a single element\n",
    "with open('TVs-all-merged.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(len(data.keys()))\n",
    "\n",
    "amount_of_descriptions = 0\n",
    "for key in data.keys():\n",
    "    amount_of_descriptions += len(data[key])\n",
    "print(amount_of_descriptions)\n",
    "\n",
    "new_data = {}\n",
    "i = 1\n",
    "for key in data.keys():\n",
    "    for description in data[key]:\n",
    "        new_data[i] = description\n",
    "        i+=1\n",
    "print(len(new_data.keys()))\n",
    "\n",
    "print(max(new_data.keys()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1352c3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list with all brands \n",
    "list_brands=[]\n",
    "\n",
    "for i in range(1,len(new_data.keys())+1):\n",
    "    if 'Brand' in new_data[i]['featuresMap']:\n",
    "        brand = new_data[i]['featuresMap']['Brand']\n",
    "        list_brands = list_brands + [brand]\n",
    "    elif 'Brand Name' in new_data[i]['featuresMap']:\n",
    "        brand2 = new_data[i]['featuresMap']['Brand Name']\n",
    "        list_brands = list_brands + [brand2]\n",
    "\n",
    "for i in range(0,len(list_brands)):\n",
    "    list_brands[i] = list_brands[i].lower()\n",
    "\n",
    "list_brands = list(dict.fromkeys(list_brands))\n",
    "list_brands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab565399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine_similarity(vec1, vec2):\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "    sum1 = sum([vec1[x] ** 2 for x in list(vec1.keys())])\n",
    "    sum2 = sum([vec2[x] ** 2 for x in list(vec2.keys())])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "\n",
    "def text_to_vector(text):\n",
    "    WORD = re.compile(r\"\\w+\")\n",
    "    words = WORD.findall(text)\n",
    "    return Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb22fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hash(num: int):\n",
    "    for i in range(0, num):\n",
    "        xvar = rd.sample(range(1, num+2),num)\n",
    "        yvar = rd.sample(range(0,num+2),num)\n",
    "    return (xvar, yvar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac3f6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_LSH_only(dataset, alltitles_list, modelID_list, bandlength):\n",
    "\n",
    "    modelID = []\n",
    "    for i in dataset:\n",
    "        modelID.append(modelID_list[i])\n",
    "\n",
    "    titles_dataset = []\n",
    "    for i in dataset:\n",
    "        titles_dataset.append(alltitles_list[i])\n",
    "    \n",
    "    complete_list_words = []\n",
    "    for i in range(0, len(titles_dataset)):\n",
    "        list_title_words = str.split(titles_dataset[i])\n",
    "        complete_list_words += list_title_words\n",
    "\n",
    "    for i in range(0,len(complete_list_words)):\n",
    "        complete_list_words[i] = complete_list_words[i].lower()    \n",
    "\n",
    "    pattern = re.compile(\"\\d+\")\n",
    "\n",
    "    for i in range(0,len(complete_list_words)-1):\n",
    "        if (complete_list_words[i+1] == 'inch' or complete_list_words[i+1] == '”' or complete_list_words[i+1] == '\"' or complete_list_words[i+1] == '-inch') and bool(pattern.match(complete_list_words[i])):\n",
    "            new_word = complete_list_words[i] + 'inch'\n",
    "            complete_list_words[i] = new_word \n",
    "        elif (complete_list_words[i+1] == 'hz' or complete_list_words[i+1] == 'hertz' or complete_list_words[i+1] == '-hz') and bool(pattern.match(complete_list_words[i])):\n",
    "            new_word2= complete_list_words[i] + 'hz'\n",
    "            complete_list_words[i] = new_word2\n",
    "    \n",
    "    new_list1 = [s.replace(\"(\", \"\") for s in complete_list_words]\n",
    "    new_list2 = [s.replace(\"[\", \"\") for s in new_list1]\n",
    "    r = re.compile(\"philips|samsung|supersonic|sharp|nec|toshiba|\\\n",
    "    hisense|sony|lg|sanyo|coby|panasonic|sansui|vizio|viewsonic|sunbritetv|haier|\\\n",
    "    proscan|jvc|pyle|lg electronics|sceptre|magnavox|mitsubishi|compaq|hannspree|\\\n",
    "    sceptre inc.|upstar|seiki|rca|craig|affinity|jvc tv|naxa|westinghouse|epson|hp|\\\n",
    "    elo|pansonic|hello kitty|sigmac|[a-z0-9]*[0-9]+[^0-9,(\\s]*|[^0-9,(\\s]*[0-9]+[a-z0-9]*\")\n",
    "    filtered_list = list(filter(r.match, new_list2))\n",
    "\n",
    "    filtered_list_final = [re.sub(r'\\\"|\\”|\\'', 'inch', item) for item in filtered_list]\n",
    "    filtered_list_final2 = [re.sub(r'-inch', 'inch', item) for item in filtered_list_final]\n",
    "    filtered_list_final3 = [re.sub(r'-hz|hertz', 'hz', item) for item in filtered_list_final2]\n",
    "    filtered_list_final4 = [s.replace(\".0\", \"\") for s in filtered_list_final3]\n",
    "    filtered_list_final5 = [s.replace(\")\", \"\") for s in filtered_list_final4]\n",
    "\n",
    "    #create list of words per tv\n",
    "    list_per_tv = []\n",
    "    for i in dataset:\n",
    "        list_per_tv.append(alltitles_list[i].split())\n",
    "\n",
    "    for i in range(0,len(list_per_tv)):\n",
    "        for j in range(0, len(list_per_tv[i])):\n",
    "            list_per_tv[i][j] = list_per_tv[i][j].lower()    \n",
    "\n",
    "    #filter words we are interested in\n",
    "    pattern = re.compile(\"\\d+\")\n",
    "                \n",
    "    for i in range(0,len(list_per_tv)):\n",
    "        for j in range(0, len(list_per_tv[i])-1):\n",
    "            if (list_per_tv[i][j+1] == 'inch' or list_per_tv[i][j+1] == '”' or list_per_tv[i][j+1] == '\"' or list_per_tv[i][j+1] == '-inch') and bool(pattern.match(list_per_tv[i][j])):\n",
    "                new_word3 = list_per_tv[i][j] + 'inch'\n",
    "                list_per_tv[i][j] = new_word3\n",
    "            elif (list_per_tv[i][j+1] == 'hz' or list_per_tv[i][j+1] == 'hertz' or list_per_tv[i][j+1] == '-hz') and bool(pattern.match(list_per_tv[i][j])):\n",
    "                new_word4 = list_per_tv[i][j] + 'hz'\n",
    "                list_per_tv[i][j] = new_word4\n",
    "\n",
    "    filtered_per_tv = []            \n",
    "    for i in range(0,len(list_per_tv)): \n",
    "        new_list3 = [s.replace(\"(\", \"\") for s in list_per_tv[i]]\n",
    "        new_list4 = [s.replace(\"[\", \"\") for s in new_list3]\n",
    "        r = re.compile(\"philips|samsung|supersonic|sharp|nec|toshiba|\\\n",
    "        hisense|sony|lg|sanyo|coby|panasonic|sansui|vizio|viewsonic|sunbritetv|haier|\\\n",
    "        proscan|jvc|pyle|lg electronics|sceptre|magnavox|mitsubishi|compaq|hannspree|\\\n",
    "        sceptre inc.|upstar|seiki|rca|craig|affinity|jvc tv|naxa|westinghouse|epson|hp|\\\n",
    "        elo|pansonic|hello kitty|sigmac|[a-z0-9]*[0-9]+[^0-9,(\\s]*|[^0-9,(\\s]*[0-9]+[a-z0-9]*\")\n",
    "        new_list5 = list(filter(r.match, new_list4))\n",
    "        filtered_per_tv.append(new_list5)\n",
    "\n",
    "    #clean the words per tv further\n",
    "    filtered_per_tv_final = []    \n",
    "    for i in range(0,len(filtered_per_tv)):\n",
    "        new_list6 = [re.sub(r'\\\"|\\”|\\'', 'inch', item) for item in filtered_per_tv[i]]\n",
    "        new_list7 = [re.sub(r'-inch', 'inch', item) for item in new_list6]\n",
    "        new_list8 = [re.sub(r'-hz|hertz', 'hz', item) for item in new_list7]\n",
    "        new_list9 = [s.replace(\".0\", \"\") for s in new_list8]\n",
    "        new_list10 = [s.replace(\")\", \"\") for s in new_list9]\n",
    "    \n",
    "        filtered_per_tv_final.append(new_list10)    \n",
    "        \n",
    "        \n",
    "    unique_filtered_list = list(dict.fromkeys(filtered_list_final5))\n",
    "\n",
    "    #create zero dataframe\n",
    "    s = (len(unique_filtered_list),len(modelID))\n",
    "    A = np.zeros(s)\n",
    "    df = pd.DataFrame(A, index=unique_filtered_list, columns=modelID)\n",
    "    \n",
    "    #create binary matrix with all model words present for each tv\n",
    "    for z in range(0, len(modelID)):\n",
    "        for j in range(0, len(filtered_per_tv_final[z])):\n",
    "            for i in range(0, len(unique_filtered_list)):\n",
    "                if filtered_per_tv_final[z][j] == unique_filtered_list[i]:\n",
    "                    df.iloc[i,z]= 1\n",
    "\n",
    "    seed(0)\n",
    "    xvar = []\n",
    "    yvar =[]\n",
    "    num_of_hashes = int(len(unique_filtered_list)/2)-int(len(unique_filtered_list)/2)%bandlength\n",
    "    primes = []\n",
    "    for number in range(num_of_hashes,1000):  \n",
    "        if number > 1:  \n",
    "            for i in range(2,number):  \n",
    "                if (number % i) == 0:  \n",
    "                    break  \n",
    "            else:  \n",
    "                primes.append(number)\n",
    "       \n",
    "    randomprime = rd.sample(primes, 1)\n",
    "    mod = randomprime[0]\n",
    "    variables = create_hash(num_of_hashes)\n",
    "    hashvalue = [ [ 0 for i in range(len(df)) ] for j in range(num_of_hashes) ]\n",
    "    for j in range(1, num_of_hashes+1):\n",
    "        for i in range(1,len(df)):\n",
    "            hashvalue[j-1][i-1] = ((variables[0][j-1]*i+variables[1][j-1])%mod)\n",
    "    final = hashvalue\n",
    "\n",
    "    transposedf = df.transpose()\n",
    "\n",
    "    sig = [float('inf')]*num_of_hashes\n",
    "    signatures = [sig]*(len(transposedf))\n",
    "    signaturesdf = pd.DataFrame(signatures, index = [modelID])\n",
    "    signaturesdf.columns = np.arange(1,num_of_hashes+1)\n",
    "\n",
    "    for i in range(0,len(transposedf)): \n",
    "        for j in range(0, len(transposedf.iloc[0])): \n",
    "            if transposedf.iloc[i,j] == 1: \n",
    "                for z in range(0, len(final)):\n",
    "                    if final[z][j] < signaturesdf.iloc[i,z]: \n",
    "                        signaturesdf.iloc[i,z] = final[z][j]                    \n",
    "\n",
    "    signaturesdft = signaturesdf.transpose()\n",
    "\n",
    "    amountofbands = num_of_hashes/bandlength\n",
    "    signaturesperband = [];\n",
    "    for i in range(1, int(amountofbands)+1):\n",
    "            if i == 1:\n",
    "                signaturesperband.append(signaturesdft.iloc[0:bandlength].astype(int))\n",
    "            else:\n",
    "                signaturesperband.append(signaturesdft.iloc[bandlength*(i-1):bandlength*(i)].astype(int))\n",
    "\n",
    "    matrixwithkeyssize = (len(signaturesperband),len(modelID))\n",
    "    matrixwithkeys = np.zeros(matrixwithkeyssize)\n",
    "    allkeys = pd.DataFrame(matrixwithkeys, columns=modelID)\n",
    "\n",
    "    string = ''\n",
    "    for b in range(0, len(signaturesperband)):\n",
    "        for j in range(0, len(signaturesperband[b].iloc[0])):\n",
    "            for i in range(0, len(signaturesperband[0])):\n",
    "                string = string + str(signaturesperband[b].iloc[i,j])\n",
    "            allkeys.iloc[b,j] = string\n",
    "            string = ''\n",
    "\n",
    "    tvlist = [\"tv\" + str(i) for i in dataset]\n",
    "\n",
    "    sizekeys = (len(allkeys.iloc[0]),len(allkeys))\n",
    "    sizething = np.zeros(sizekeys)\n",
    "    allkeyspertv = pd.DataFrame(sizething, index=modelID)\n",
    "\n",
    "    tvlength = (len(tvlist),len(tvlist))\n",
    "    Atvlength = np.zeros(tvlength)\n",
    "    dftvlist = pd.DataFrame(Atvlength, index=tvlist, columns=tvlist)\n",
    "    dftvlistint = dftvlist.astype(int)\n",
    "\n",
    "    for i in range(0, len(allkeys)):\n",
    "        allkeysperband = pd.DataFrame(allkeys.iloc[i,:])\n",
    "        allkeysperband.reset_index(drop=True)\n",
    "        allkeysperband.index = tvlist\n",
    "        sortedkeypertv = allkeysperband.sort_values(i)\n",
    "\n",
    "        masterlist = [];\n",
    "        smallerlist = [];\n",
    "        for j in range(0, len(sortedkeypertv)-1):\n",
    "            if j == (len(sortedkeypertv)-2):\n",
    "                smallerlist.append(str(sortedkeypertv.index[j]))\n",
    "                smallerlist.append(str(sortedkeypertv.index[j+1]))\n",
    "                masterlist.append(smallerlist)\n",
    "            elif sortedkeypertv[i][j+1] == sortedkeypertv[i][j]:\n",
    "                smallerlist.append(str(sortedkeypertv.index[j]))\n",
    "            else:\n",
    "                smallerlist.append(str(sortedkeypertv.index[j]))\n",
    "                masterlist.append(smallerlist)\n",
    "                smallerlist = []\n",
    "\n",
    "        listofcombos = []\n",
    "        for z in range(0, len(masterlist)):\n",
    "            for subset in itertools.combinations(masterlist[z], 2):\n",
    "                listofcombos.append(subset)\n",
    "\n",
    "        for p in range(0,len(listofcombos)):\n",
    "            dftvlistint[listofcombos[p][0]][listofcombos[p][1]] = 1\n",
    "            dftvlistint[listofcombos[p][1]][listofcombos[p][0]] = 1\n",
    "\n",
    "    ## Nu heb je LSH afgerond\n",
    "\n",
    "    arrdata = dftvlistint.to_numpy()\n",
    "    newarrdata = np.triu(arrdata)\n",
    "    dfnew = pd.DataFrame(newarrdata)\n",
    "    \n",
    "    # numofcomp2 = dftvlistint\n",
    "    #All comparisons made from candidate pairs\n",
    "    numofcomp2 = dfnew\n",
    "\n",
    "    numofcomp2 = numofcomp2.replace(float('-inf'),0)\n",
    "    numberofcomparisons2 = sum(numofcomp2[numofcomp2 > 0].count())\n",
    "    \n",
    "    ## All comparisons possible\n",
    "    onematrixl = (len(dftvlistint), len(dftvlistint))\n",
    "    onematrix = np.ones(onematrixl)\n",
    "    onematrixup = np.triu(onematrix)\n",
    "    np.fill_diagonal(onematrixup, 0)\n",
    "    onematrixupdf = pd.DataFrame(onematrixup)\n",
    "    numofonesindf = sum(onematrixupdf[onematrixupdf > 0].count())\n",
    "    \n",
    "    fracofcomparisons = numberofcomparisons2/numofonesindf\n",
    "\n",
    "    filtered_final = []\n",
    "    regex = re.compile(r'.{3,}inch')\n",
    "    for i in range(0, len(filtered_per_tv_final)):\n",
    "        filtered = [g for g in filtered_per_tv_final[i] if not regex.match(g)]\n",
    "        filtered_final.append(filtered)\n",
    "\n",
    "    shops = []\n",
    "    for i in dataset:\n",
    "         shops.append(new_data[i+1][\"shop\"])\n",
    "    \n",
    "    brands = []\n",
    "    for i in dataset:\n",
    "        if ('Brand' in new_data[i+1]['featuresMap']):\n",
    "            brands.append(new_data[i+1]['featuresMap']['Brand'])\n",
    "        elif ('Brand Name' in new_data[i+1]['featuresMap']):\n",
    "            brands.append(new_data[i+1]['featuresMap']['Brand Name'])\n",
    "        else:\n",
    "            brands.append('no result')\n",
    "    for i in range(0,len(brands)):\n",
    "        brands[i] = brands[i].lower() \n",
    "    brandslist = [re.sub(r'lg electronics', 'lg', item) for item in brands]\n",
    "    brandslist2 = [re.sub(r'sceptre inc.', 'sceptre', item) for item in brandslist]\n",
    "    brands_final = [re.sub(r'jvc tv', 'jvc', item) for item in brandslist2]\n",
    "\n",
    "    tvlength = (len(tvlist),len(tvlist))\n",
    "    Atvlength = np.zeros(tvlength)\n",
    "    dftvlist = pd.DataFrame(Atvlength, index=tvlist, columns=tvlist )\n",
    "    dftvlist\n",
    "\n",
    "    for i in range(0, len(dataset)):\n",
    "        for j in range(0, len(dataset)):\n",
    "            if j > i:\n",
    "                if dfnew.iloc[i,j] == 1:\n",
    "                    dftvlist.iloc[i,j] = get_cosine_similarity(text_to_vector(titles_dataset[i]), text_to_vector(titles_dataset[j]))\n",
    "    \n",
    "    for i in range(0, len(dftvlist)):\n",
    "        for j in range(0, len(dftvlist)):\n",
    "            if j > i and shops[i] == shops[j]:\n",
    "                dftvlist.iloc[i,j] = -float('inf')\n",
    "            if j > i and brands_final[i] != brands_final[j] and brands_final[i] != 'no result' and brands_final[j] != 'no result':\n",
    "                dftvlist.iloc[i,j] = -float('inf')\n",
    "                \n",
    "    ## All comparisons, with shops and brands taken into account with LSH\n",
    "    numofcomp = dftvlist\n",
    "    numofcomp = numofcomp.replace(float('-inf'),0)\n",
    "    numberofcomparisons = sum(numofcomp[numofcomp > 0].count())\n",
    "    \n",
    "    ## All possible comparisons, with shops and brands taken into account\n",
    "    onematrix2 = (len(dftvlistint), len(dftvlistint))\n",
    "    onematrix3 = np.ones(onematrix2)\n",
    "    onematrixup2 = np.triu(onematrix3)\n",
    "    np.fill_diagonal(onematrixup2, 0)\n",
    "    onematrixupdf2 = pd.DataFrame(onematrixup2)\n",
    "    \n",
    "    for i in range(0, len(onematrixupdf2)):\n",
    "        for j in range(0, len(onematrixupdf2)):\n",
    "            if j > i and shops[i] == shops[j]:\n",
    "                onematrixupdf2.iloc[i,j] = 0\n",
    "            if j > i and brands_final[i] != brands_final[j] and brands_final[i] != 'no result' and brands_final[j] != 'no result':\n",
    "                onematrixupdf2.iloc[i,j] = 0\n",
    "                \n",
    "    numofonesindf4 = sum(onematrixupdf[onematrixupdf2 > 0].count())\n",
    "    \n",
    "    fracofcomparisons2 = numberofcomparisons/numofonesindf4\n",
    "    \n",
    "    originalm = (len(modelID),len(modelID))\n",
    "    originalA = np.zeros(originalm)\n",
    "    originalcorrect = pd.DataFrame(originalA, index=modelID, columns=modelID)\n",
    "    for i in range(0, len(modelID)):\n",
    "        for j in range(0, len(modelID)):\n",
    "            if modelID[i] == modelID[j]:\n",
    "                originalcorrect.iloc[i,j] = 1\n",
    "    originalcorrect.values[[np.arange(originalcorrect.shape[0])]*2] = 0\n",
    "    \n",
    "    falsenegatives_lsh = 0\n",
    "    falsepositives_lsh = 0\n",
    "    truenegatives_lsh = 0\n",
    "    truepositives_lsh = 0\n",
    "\n",
    "    for i in range(0, len(originalcorrect)):\n",
    "        for j in range(0, len(originalcorrect)):\n",
    "            if j > i:\n",
    "                if  dfnew.iloc[i,j] == 0 and originalcorrect.iloc[i,j] == 0:\n",
    "                    truenegatives_lsh = truenegatives_lsh + 1\n",
    "                elif  dfnew.iloc[i,j] == 0 and originalcorrect.iloc[i,j] == 1:\n",
    "                    falsenegatives_lsh = falsenegatives_lsh + 1\n",
    "                elif  dfnew.iloc[i,j] == 1 and originalcorrect.iloc[i,j] == 0:\n",
    "                    falsepositives_lsh = falsepositives_lsh + 1\n",
    "                elif  dfnew.iloc[i,j] == 1 and originalcorrect.iloc[i,j] == 1:\n",
    "                    truepositives_lsh = truepositives_lsh + 1\n",
    "\n",
    "    f1measure_lsh = (truepositives_lsh/(truepositives_lsh+0.5*(falsenegatives_lsh+falsepositives_lsh)))\n",
    "\n",
    "    pairquality_lsh = (truepositives_lsh)/(numberofcomparisons) \n",
    "    paircompleteness_lsh = (truepositives_lsh)/(truepositives_lsh + falsenegatives_lsh)\n",
    "  \n",
    "         #print(\"Pair quality = \", str(pairquality))\n",
    "#     print(\"Pair completeness = \", str(paircompleteness))\n",
    "    f1starmeasure_lsh = (2*pairquality_lsh*paircompleteness_lsh)/(pairquality_lsh+paircompleteness_lsh)\n",
    "#     print(\"f1starmeasure = \" + str(f1starmeasure))\n",
    "    \n",
    "    return (fracofcomparisons, fracofcomparisons2, f1starmeasure_lsh, f1measure_lsh, truepositives_lsh, falsepositives_lsh, falsenegatives_lsh, truenegatives_lsh, pairquality_lsh, paircompleteness_lsh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f2b213",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_LSH(dataset, alltitles_list, modelID_list, bandlength, cos_threshold, beta, delta, clustering_threshold):\n",
    "\n",
    "    modelID = []\n",
    "    for i in dataset:\n",
    "        modelID.append(modelID_list[i])\n",
    "\n",
    "    titles_dataset = []\n",
    "    for i in dataset:\n",
    "        titles_dataset.append(alltitles_list[i])\n",
    "    \n",
    "    complete_list_words = []\n",
    "    for i in range(0, len(titles_dataset)):\n",
    "        list_title_words = str.split(titles_dataset[i])\n",
    "        complete_list_words += list_title_words\n",
    "\n",
    "    for i in range(0,len(complete_list_words)):\n",
    "        complete_list_words[i] = complete_list_words[i].lower()    \n",
    "\n",
    "    pattern = re.compile(\"\\d+\")\n",
    "\n",
    "    for i in range(0,len(complete_list_words)-1):\n",
    "        if (complete_list_words[i+1] == 'inch' or complete_list_words[i+1] == '”' or complete_list_words[i+1] == '\"' or complete_list_words[i+1] == '-inch') and bool(pattern.match(complete_list_words[i])):\n",
    "            new_word = complete_list_words[i] + 'inch'\n",
    "            complete_list_words[i] = new_word \n",
    "        elif (complete_list_words[i+1] == 'hz' or complete_list_words[i+1] == 'hertz' or complete_list_words[i+1] == '-hz') and bool(pattern.match(complete_list_words[i])):\n",
    "            new_word2= complete_list_words[i] + 'hz'\n",
    "            complete_list_words[i] = new_word2\n",
    "    \n",
    "    new_list1 = [s.replace(\"(\", \"\") for s in complete_list_words]\n",
    "    new_list2 = [s.replace(\"[\", \"\") for s in new_list1]\n",
    "    r = re.compile(\"philips|samsung|supersonic|sharp|nec|toshiba|\\\n",
    "    hisense|sony|lg|sanyo|coby|panasonic|sansui|vizio|viewsonic|sunbritetv|haier|\\\n",
    "    proscan|jvc|pyle|lg electronics|sceptre|magnavox|mitsubishi|compaq|hannspree|\\\n",
    "    sceptre inc.|upstar|seiki|rca|craig|affinity|jvc tv|naxa|westinghouse|epson|hp|\\\n",
    "    elo|pansonic|hello kitty|sigmac|[a-z0-9]*[0-9]+[^0-9,(\\s]*|[^0-9,(\\s]*[0-9]+[a-z0-9]*\")\n",
    "    filtered_list = list(filter(r.match, new_list2))\n",
    "\n",
    "    filtered_list_final = [re.sub(r'\\\"|\\”|\\'', 'inch', item) for item in filtered_list]\n",
    "    filtered_list_final2 = [re.sub(r'-inch', 'inch', item) for item in filtered_list_final]\n",
    "    filtered_list_final3 = [re.sub(r'-hz|hertz', 'hz', item) for item in filtered_list_final2]\n",
    "    filtered_list_final4 = [s.replace(\".0\", \"\") for s in filtered_list_final3]\n",
    "    filtered_list_final5 = [s.replace(\")\", \"\") for s in filtered_list_final4]\n",
    "\n",
    "    #create list of words per tv\n",
    "    list_per_tv = []\n",
    "    for i in dataset:\n",
    "        list_per_tv.append(alltitles_list[i].split())\n",
    "\n",
    "    for i in range(0,len(list_per_tv)):\n",
    "        for j in range(0, len(list_per_tv[i])):\n",
    "            list_per_tv[i][j] = list_per_tv[i][j].lower()    \n",
    "\n",
    "    #filter words we are interested in\n",
    "    pattern = re.compile(\"\\d+\")\n",
    "                \n",
    "    for i in range(0,len(list_per_tv)):\n",
    "        for j in range(0, len(list_per_tv[i])-1):\n",
    "            if (list_per_tv[i][j+1] == 'inch' or list_per_tv[i][j+1] == '”' or list_per_tv[i][j+1] == '\"' or list_per_tv[i][j+1] == '-inch') and bool(pattern.match(list_per_tv[i][j])):\n",
    "                new_word3 = list_per_tv[i][j] + 'inch'\n",
    "                list_per_tv[i][j] = new_word3\n",
    "            elif (list_per_tv[i][j+1] == 'hz' or list_per_tv[i][j+1] == 'hertz' or list_per_tv[i][j+1] == '-hz') and bool(pattern.match(list_per_tv[i][j])):\n",
    "                new_word4 = list_per_tv[i][j] + 'hz'\n",
    "                list_per_tv[i][j] = new_word4\n",
    "\n",
    "    filtered_per_tv = []            \n",
    "    for i in range(0,len(list_per_tv)): \n",
    "        new_list3 = [s.replace(\"(\", \"\") for s in list_per_tv[i]]\n",
    "        new_list4 = [s.replace(\"[\", \"\") for s in new_list3]\n",
    "        r = re.compile(\"philips|samsung|supersonic|sharp|nec|toshiba|\\\n",
    "        hisense|sony|lg|sanyo|coby|panasonic|sansui|vizio|viewsonic|sunbritetv|haier|\\\n",
    "        proscan|jvc|pyle|lg electronics|sceptre|magnavox|mitsubishi|compaq|hannspree|\\\n",
    "        sceptre inc.|upstar|seiki|rca|craig|affinity|jvc tv|naxa|westinghouse|epson|hp|\\\n",
    "        elo|pansonic|hello kitty|sigmac|[a-z0-9]*[0-9]+[^0-9,(\\s]*|[^0-9,(\\s]*[0-9]+[a-z0-9]*\")\n",
    "        new_list5 = list(filter(r.match, new_list4))\n",
    "        filtered_per_tv.append(new_list5)\n",
    "\n",
    "    #clean the words per tv further\n",
    "    filtered_per_tv_final = []    \n",
    "    for i in range(0,len(filtered_per_tv)):\n",
    "        new_list6 = [re.sub(r'\\\"|\\”|\\'', 'inch', item) for item in filtered_per_tv[i]]\n",
    "        new_list7 = [re.sub(r'-inch', 'inch', item) for item in new_list6]\n",
    "        new_list8 = [re.sub(r'-hz|hertz', 'hz', item) for item in new_list7]\n",
    "        new_list9 = [s.replace(\".0\", \"\") for s in new_list8]\n",
    "        new_list10 = [s.replace(\")\", \"\") for s in new_list9]\n",
    "    \n",
    "        filtered_per_tv_final.append(new_list10)    \n",
    "        \n",
    "        \n",
    "    unique_filtered_list = list(dict.fromkeys(filtered_list_final5))\n",
    "\n",
    "    #create zero dataframe\n",
    "    s = (len(unique_filtered_list),len(modelID))\n",
    "    A = np.zeros(s)\n",
    "    df = pd.DataFrame(A, index=unique_filtered_list, columns=modelID)\n",
    "    \n",
    "    #create binary matrix with all model words present for each tv\n",
    "    for z in range(0, len(modelID)):\n",
    "        for j in range(0, len(filtered_per_tv_final[z])):\n",
    "            for i in range(0, len(unique_filtered_list)):\n",
    "                if filtered_per_tv_final[z][j] == unique_filtered_list[i]:\n",
    "                    df.iloc[i,z]= 1\n",
    "\n",
    "    seed(0)\n",
    "    xvar = []\n",
    "    yvar =[]\n",
    "    num_of_hashes = int(len(unique_filtered_list)/2)-int(len(unique_filtered_list)/2)%bandlength\n",
    "    primes = []\n",
    "    for number in range(num_of_hashes,1000):  \n",
    "        if number > 1:  \n",
    "            for i in range(2,number):  \n",
    "                if (number % i) == 0:  \n",
    "                    break  \n",
    "            else:  \n",
    "                primes.append(number)\n",
    "       \n",
    "    randomprime = rd.sample(primes, 1)\n",
    "    mod = randomprime[0]\n",
    "    variables = create_hash(num_of_hashes)\n",
    "    hashvalue = [ [ 0 for i in range(len(df)) ] for j in range(num_of_hashes) ]\n",
    "    for j in range(1, num_of_hashes+1):\n",
    "        for i in range(1,len(df)):\n",
    "            hashvalue[j-1][i-1] = ((variables[0][j-1]*i+variables[1][j-1])%mod)\n",
    "    final = hashvalue\n",
    "\n",
    "    transposedf = df.transpose()\n",
    "\n",
    "    sig = [float('inf')]*num_of_hashes\n",
    "    signatures = [sig]*(len(transposedf))\n",
    "    signaturesdf = pd.DataFrame(signatures, index = [modelID])\n",
    "    signaturesdf.columns = np.arange(1,num_of_hashes+1)\n",
    "\n",
    "    for i in range(0,len(transposedf)): #\n",
    "        for j in range(0, len(transposedf.iloc[0])): \n",
    "            if transposedf.iloc[i,j] == 1: \n",
    "                for z in range(0, len(final)): \n",
    "                    if final[z][j] < signaturesdf.iloc[i,z]:\n",
    "                        signaturesdf.iloc[i,z] = final[z][j]                    \n",
    "\n",
    "    signaturesdft = signaturesdf.transpose()\n",
    "\n",
    "    amountofbands = num_of_hashes/bandlength\n",
    "    signaturesperband = [];\n",
    "    for i in range(1, int(amountofbands)+1):\n",
    "            if i == 1:\n",
    "                signaturesperband.append(signaturesdft.iloc[0:bandlength].astype(int))\n",
    "            else:\n",
    "                signaturesperband.append(signaturesdft.iloc[bandlength*(i-1):bandlength*(i)].astype(int))\n",
    "\n",
    "    matrixwithkeyssize = (len(signaturesperband),len(modelID))\n",
    "    matrixwithkeys = np.zeros(matrixwithkeyssize)\n",
    "    allkeys = pd.DataFrame(matrixwithkeys, columns=modelID)\n",
    "\n",
    "    string = ''\n",
    "    for b in range(0, len(signaturesperband)):\n",
    "        for j in range(0, len(signaturesperband[b].iloc[0])):\n",
    "            for i in range(0, len(signaturesperband[0])):\n",
    "                string = string + str(signaturesperband[b].iloc[i,j])\n",
    "            allkeys.iloc[b,j] = string\n",
    "            string = ''\n",
    "\n",
    "    tvlist = [\"tv\" + str(i) for i in dataset]\n",
    "\n",
    "    sizekeys = (len(allkeys.iloc[0]),len(allkeys))\n",
    "    sizething = np.zeros(sizekeys)\n",
    "    allkeyspertv = pd.DataFrame(sizething, index=modelID)\n",
    "\n",
    "    tvlength = (len(tvlist),len(tvlist))\n",
    "    Atvlength = np.zeros(tvlength)\n",
    "    dftvlist = pd.DataFrame(Atvlength, index=tvlist, columns=tvlist)\n",
    "    dftvlistint = dftvlist.astype(int)\n",
    "\n",
    "    for i in range(0, len(allkeys)):\n",
    "        allkeysperband = pd.DataFrame(allkeys.iloc[i,:])\n",
    "        allkeysperband.reset_index(drop=True)\n",
    "        allkeysperband.index = tvlist\n",
    "        sortedkeypertv = allkeysperband.sort_values(i)\n",
    "\n",
    "        masterlist = [];\n",
    "        smallerlist = [];\n",
    "        for j in range(0, len(sortedkeypertv)-1):\n",
    "            if j == (len(sortedkeypertv)-2):\n",
    "                smallerlist.append(str(sortedkeypertv.index[j]))\n",
    "                smallerlist.append(str(sortedkeypertv.index[j+1]))\n",
    "                masterlist.append(smallerlist)\n",
    "            elif sortedkeypertv[i][j+1] == sortedkeypertv[i][j]:\n",
    "                smallerlist.append(str(sortedkeypertv.index[j]))\n",
    "            else:\n",
    "                smallerlist.append(str(sortedkeypertv.index[j]))\n",
    "                masterlist.append(smallerlist)\n",
    "                smallerlist = []\n",
    "\n",
    "        listofcombos = []\n",
    "        for z in range(0, len(masterlist)):\n",
    "            for subset in itertools.combinations(masterlist[z], 2):\n",
    "                listofcombos.append(subset)\n",
    "\n",
    "        for p in range(0,len(listofcombos)):\n",
    "            dftvlistint[listofcombos[p][0]][listofcombos[p][1]] = 1\n",
    "            dftvlistint[listofcombos[p][1]][listofcombos[p][0]] = 1\n",
    "\n",
    "    #LSH finished\n",
    "\n",
    "    arrdata = dftvlistint.to_numpy()\n",
    "    newarrdata = np.triu(arrdata)\n",
    "    dfnew = pd.DataFrame(newarrdata)\n",
    "    \n",
    "\n",
    "    numofcomp2 = dfnew\n",
    "\n",
    "    numofcomp2 = numofcomp2.replace(float('-inf'),0)\n",
    "    numberofcomparisons2 = sum(numofcomp2[numofcomp2 > 0].count())\n",
    "    \n",
    "    ## All comparisons possible\n",
    "    onematrixl = (len(dftvlistint), len(dftvlistint))\n",
    "    onematrix = np.ones(onematrixl)\n",
    "    onematrixup = np.triu(onematrix)\n",
    "    np.fill_diagonal(onematrixup, 0)\n",
    "    onematrixupdf = pd.DataFrame(onematrixup)\n",
    "    numofonesindf = sum(onematrixupdf[onematrixupdf > 0].count())\n",
    "    \n",
    "    fracofcomparisons = numberofcomparisons2/numofonesindf\n",
    "\n",
    "    filtered_final = []\n",
    "    regex = re.compile(r'.{3,}inch')\n",
    "    for i in range(0, len(filtered_per_tv_final)):\n",
    "        filtered = [g for g in filtered_per_tv_final[i] if not regex.match(g)]\n",
    "        filtered_final.append(filtered)\n",
    "\n",
    "    shops = []\n",
    "    for i in dataset:\n",
    "         shops.append(new_data[i+1][\"shop\"])\n",
    "    \n",
    "    brands = []\n",
    "    for i in dataset:\n",
    "        if ('Brand' in new_data[i+1]['featuresMap']):\n",
    "            brands.append(new_data[i+1]['featuresMap']['Brand'])\n",
    "        elif ('Brand Name' in new_data[i+1]['featuresMap']):\n",
    "            brands.append(new_data[i+1]['featuresMap']['Brand Name'])\n",
    "        else:\n",
    "            brands.append('no result')\n",
    "    for i in range(0,len(brands)):\n",
    "        brands[i] = brands[i].lower() \n",
    "    brandslist = [re.sub(r'lg electronics', 'lg', item) for item in brands]\n",
    "    brandslist2 = [re.sub(r'sceptre inc.', 'sceptre', item) for item in brandslist]\n",
    "    brands_final = [re.sub(r'jvc tv', 'jvc', item) for item in brandslist2]\n",
    "\n",
    "    tvlength = (len(tvlist),len(tvlist))\n",
    "    Atvlength = np.zeros(tvlength)\n",
    "    dftvlist = pd.DataFrame(Atvlength, index=tvlist, columns=tvlist )\n",
    "    dftvlist\n",
    "\n",
    "    for i in range(0, len(dataset)):\n",
    "        for j in range(0, len(dataset)):\n",
    "            if j > i:\n",
    "                if dfnew.iloc[i,j] == 1:\n",
    "                    dftvlist.iloc[i,j] = get_cosine_similarity(text_to_vector(titles_dataset[i]), text_to_vector(titles_dataset[j]))\n",
    "    \n",
    "    for i in range(0, len(dftvlist)):\n",
    "        for j in range(0, len(dftvlist)):\n",
    "            if j > i and shops[i] == shops[j]:\n",
    "                dftvlist.iloc[i,j] = -float('inf')\n",
    "            if j > i and brands_final[i] != brands_final[j] and brands_final[i] != 'no result' and brands_final[j] != 'no result':\n",
    "                dftvlist.iloc[i,j] = -float('inf')\n",
    "                \n",
    "    ## All comparisons, with shops and brands taken into account with LSH\n",
    "    numofcomp = dftvlist\n",
    "    numofcomp = numofcomp.replace(float('-inf'),0)\n",
    "    numberofcomparisons = sum(numofcomp[numofcomp > 0].count())\n",
    "    \n",
    "    ## All possible comparisons, with shops and brands taken into account\n",
    "    onematrix2 = (len(dftvlistint), len(dftvlistint))\n",
    "    onematrix3 = np.ones(onematrix2)\n",
    "    onematrixup2 = np.triu(onematrix3)\n",
    "    np.fill_diagonal(onematrixup2, 0)\n",
    "    onematrixupdf2 = pd.DataFrame(onematrixup2)\n",
    "    \n",
    "    for i in range(0, len(onematrixupdf2)):\n",
    "        for j in range(0, len(onematrixupdf2)):\n",
    "            if j > i and shops[i] == shops[j]:\n",
    "                onematrixupdf2.iloc[i,j] = 0\n",
    "            if j > i and brands_final[i] != brands_final[j] and brands_final[i] != 'no result' and brands_final[j] != 'no result':\n",
    "                onematrixupdf2.iloc[i,j] = 0\n",
    "                \n",
    "    numofonesindf4 = sum(onematrixupdf[onematrixupdf2 > 0].count())\n",
    "    \n",
    "    fracofcomparisons2 = numberofcomparisons/numofonesindf4\n",
    "    \n",
    "    \n",
    "    tvlength = (len(tvlist),len(tvlist))\n",
    "    Atvlength = np.zeros(tvlength)\n",
    "    dfsimlev = pd.DataFrame(Atvlength)\n",
    "\n",
    "    tvlength1 = (len(tvlist),len(tvlist))\n",
    "    Atvlength1 = np.zeros(tvlength1)\n",
    "    dfsimlevMW = pd.DataFrame(Atvlength1)\n",
    "\n",
    "    list1=[]\n",
    "    regex = re.compile(r'[0-9]*')\n",
    "    for i in range(0, len(filtered_final)):\n",
    "        list2 = [re.sub(r'[0-9]*', '', item) for item in filtered_final[i]]\n",
    "        list1.append(list2)\n",
    "\n",
    "    list3=[]\n",
    "    regex = re.compile(r'[^0-9]*')\n",
    "    for i in range(0, len(filtered_final)):\n",
    "        list4 = [re.sub(r'[^0-9]*', '', item) for item in filtered_final[i]]\n",
    "        list3.append(list4)\n",
    "\n",
    "    #TMWM\n",
    "    sum_lev_MW = 0\n",
    "    sum_lev = 0\n",
    "    sum_length = 0\n",
    "    sum_length_allpairs = 0\n",
    "    for i in range(0, len(dftvlist)):\n",
    "        for j in range(0, len(dftvlist)):\n",
    "            if j > i and dfnew.iloc[i,j] == 1 and dftvlist.iloc[i,j] < cos_threshold and dftvlist.iloc[i,j] >= 0:\n",
    "                for z in range(0, len(filtered_final[i])):\n",
    "                    for x in range(0, len(filtered_final[j])):\n",
    "                        first_model = list1[i][z]\n",
    "                        second_model = list1[j][x]\n",
    "                        text_distance = textdistance.levenshtein.normalized_distance(first_model, second_model)\n",
    "                        first_item = filtered_final[i][z]\n",
    "                        second_item = filtered_final[j][x]\n",
    "                        total_distance = textdistance.levenshtein.normalized_distance(first_item, second_item)\n",
    "                        length_allpairs = len(filtered_final[i][z])+len(filtered_final[j][x])\n",
    "                        sum_lev = sum_lev + (1-total_distance) * length_allpairs\n",
    "                        sum_length_allpairs = sum_length_allpairs + length_allpairs\n",
    "                        if text_distance < 0.1:\n",
    "                            first_word = list3[i][z]\n",
    "                            second_word = list3[j][x]\n",
    "                            numeric_distance = textdistance.levenshtein.normalized_distance(first_word, second_word)\n",
    "                            if numeric_distance > 0:\n",
    "                                dftvlist.iloc[i,j] = -float('inf')\n",
    "                                length = len(filtered_final[i][z])+len(filtered_final[j][x])\n",
    "                                sum_length = sum_length + length\n",
    "                            else:\n",
    "                                first_string = filtered_final[i][z]\n",
    "                                second_string = filtered_final[j][x]\n",
    "                                string_distance = textdistance.levenshtein.normalized_distance(first_string, second_string)\n",
    "                                length = len(filtered_final[i][z])+len(filtered_final[j][x])\n",
    "                                sum_lev_MW = sum_lev_MW + (1-string_distance) * length\n",
    "                                sum_length = sum_length + length\n",
    "            if dftvlist.iloc[i,j] != 0 and sum_length != 0:\n",
    "                avg_lev = sum_lev / sum_length_allpairs\n",
    "                avg_lev_MW = sum_lev_MW / sum_length\n",
    "                dfsimlev.iloc[i,j] = beta * dftvlist.iloc[i,j] + (1-beta) * avg_lev\n",
    "                dfsimlevMW.iloc[i,j] = delta * avg_lev_MW + (1-delta) * dfsimlev.iloc[i,j]\n",
    "                sum_lev_MW = 0\n",
    "                sum_length = 0\n",
    "                sum_lev = 0\n",
    "                sum_length_allpairs = 0\n",
    "\n",
    "    for i in range(0, len(dfsimlevMW)):\n",
    "        for j in range(0, len(dfsimlevMW)):\n",
    "            if j > i and shops[i] == shops[j]:\n",
    "                dfsimlevMW.iloc[i,j] = -float('inf')\n",
    "            if j > i and brands_final[i] != brands_final[j] and brands_final[i] != 'no result' and brands_final[j] != 'no result':\n",
    "                dfsimlevMW.iloc[i,j] = -float('inf')\n",
    "   \n",
    "    \n",
    "    finalm = (len(dfsimlevMW),len(dfsimlevMW))\n",
    "    finalA = np.ones(finalm)\n",
    "    onematrix = pd.DataFrame(finalA)\n",
    "    dfsimlevMWone = onematrix.sub(dfsimlevMW)\n",
    "    finaldissimilaritymatrix = dfsimlevMWone.replace(1, 1000)\n",
    "    finaldissimilaritymatrix2 = finaldissimilaritymatrix.replace(float('inf'), 1000)\n",
    "    finaldissimilaritymatrix2numpy = finaldissimilaritymatrix2.to_numpy()\n",
    "    i_lower = np.tril_indices(len(tvlist), -1)\n",
    "    finaldissimilaritymatrix2numpy[i_lower] = finaldissimilaritymatrix2numpy.T[i_lower]\n",
    "    dffinaldissim = pd.DataFrame(finaldissimilaritymatrix2numpy)\n",
    "\n",
    "    clustering = AgglomerativeClustering(n_clusters = None, affinity='precomputed', linkage='complete', distance_threshold=clustering_threshold, compute_full_tree= True).fit(dffinaldissim)\n",
    "    labelsclust = clustering.labels_\n",
    "    \n",
    "    originalm = (len(modelID),len(modelID))\n",
    "    originalA = np.zeros(originalm)\n",
    "    originalcorrect = pd.DataFrame(originalA, index=modelID, columns=modelID)\n",
    "    for i in range(0, len(modelID)):\n",
    "        for j in range(0, len(modelID)):\n",
    "            if modelID[i] == modelID[j]:\n",
    "                originalcorrect.iloc[i,j] = 1\n",
    "    originalcorrect.values[[np.arange(originalcorrect.shape[0])]*2] = 0\n",
    "\n",
    "    clusterdm = (len(modelID),len(modelID))\n",
    "    clusteredA = np.zeros(clusterdm)\n",
    "    clusteredmat = pd.DataFrame(clusteredA, index=modelID, columns=modelID)\n",
    "    check = pd.DataFrame(list(zip(labelsclust, modelID)))\n",
    "    for i in range(0, len(modelID)):\n",
    "        for j in range(0, len(modelID)):\n",
    "            if check[0][i] == check[0][j]:\n",
    "                clusteredmat.iloc[i,j] = 1\n",
    "    clusteredmat.values[[np.arange(clusteredmat.shape[0])]*2] = 0\n",
    "    #calculate performance measures\n",
    "    falsenegatives = 0\n",
    "    falsepositives = 0\n",
    "    truenegatives = 0\n",
    "    truepositives = 0\n",
    "    \n",
    "    for i in range(0, len(originalcorrect)):\n",
    "        for j in range(0, len(originalcorrect)):\n",
    "            if j > i:\n",
    "                if  clusteredmat.iloc[i,j] == 0 and originalcorrect.iloc[i,j] == 0:\n",
    "                    truenegatives = truenegatives + 1\n",
    "                elif  clusteredmat.iloc[i,j] == 0 and originalcorrect.iloc[i,j] == 1:\n",
    "                    falsenegatives = falsenegatives + 1\n",
    "                elif  clusteredmat.iloc[i,j] == 1 and originalcorrect.iloc[i,j] == 0:\n",
    "                    falsepositives = falsepositives + 1\n",
    "                elif  clusteredmat.iloc[i,j] == 1 and originalcorrect.iloc[i,j] == 1:\n",
    "                    truepositives = truepositives + 1\n",
    "\n",
    "#     print(\"truepositives = \"+ str(truepositives))\n",
    "#     print(\"truenegatives = \" + str(truenegatives))\n",
    "#     print(\"falsepositives = \" + str(falsepositives))\n",
    "#     print(\"falsenegatives = \"+ str(falsenegatives))\n",
    "    f1measure = (truepositives/(truepositives+0.5*(falsenegatives+falsepositives)))\n",
    "#     print(\"f1measure = \" + str(f1measure))\n",
    "\n",
    "    pairquality = (truepositives)/(numberofcomparisons) \n",
    "    paircompleteness = (truepositives)/(truepositives + falsenegatives)\n",
    "  \n",
    "         #print(\"Pair quality = \", str(pairquality))\n",
    "#     print(\"Pair completeness = \", str(paircompleteness))\n",
    "#     f1starmeasure = (2*pairquality*paircompleteness)/(pairquality+paircompleteness)\n",
    "#     print(\"f1starmeasure = \" + str(f1starmeasure))\n",
    "    \n",
    "    return (fracofcomparisons, fracofcomparisons2, f1measure, truepositives, falsepositives, falsenegatives, truenegatives, pairquality, paircompleteness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907241ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list with all model ID's\n",
    "modelID_list = []\n",
    "for i in range(1, len(new_data)+1):\n",
    "    modelID_list.append(new_data[i][\"modelID\"])\n",
    "    \n",
    "# for i in range(0,len(modelID_list)):\n",
    "#     modelID_list[i] = modelID_list[i].lower()    \n",
    "modelID_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ce1b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create train and test set for each bootstrap iteration\n",
    "bootstraps = 5\n",
    "trainlist = []\n",
    "testlist = []\n",
    "for i in range(0, bootstraps):\n",
    "    trainsample = rd.choices(range(0,len(modelID_list)),k=len(modelID_list))\n",
    "    trainsample2 = list(dict.fromkeys(trainsample))\n",
    "    trainset = sorted(trainsample2)\n",
    "    full_data = list(range(0, len(modelID_list)))\n",
    "    testset = list(set(full_data) - set(trainset))\n",
    "    trainlist.append(list(trainset))\n",
    "    testlist.append(list(testset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acb3ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list with all titles\n",
    "titles = []\n",
    "for i in range(1, len(new_data.keys())+1):\n",
    "     titles.append(new_data[i][\"title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfe8755",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "list_f1star = []\n",
    "list_f1 = []\n",
    "list_compare = []\n",
    "list_compare2 = []\n",
    "list_pair_qual = []\n",
    "list_pair_compl= []\n",
    "list_truepos = []\n",
    "list_falsepos = []\n",
    "list_falseneg = []\n",
    "list_trueneg = []\n",
    "list_f1star_test = []\n",
    "list_f1_test = []\n",
    "list_compare_test = []\n",
    "list_compare2_test = []\n",
    "list_pair_qual_test = []\n",
    "list_pair_compl_test = []\n",
    "list_truepos_test = []\n",
    "list_falsepos_test = []\n",
    "list_falseneg_test = []\n",
    "list_trueneg_test = []\n",
    "#create random vectors for different parameter values\n",
    "bandlength =  [1]\n",
    "cos_threshold = [0.7, 0.8, 0.9]\n",
    "beta =  [0.2, 0.3, 0.4] \n",
    "delta = [0.8, 0.9]\n",
    "clustering_threshold = [0.2, 0.15, 0.17, 0.22]\n",
    "\n",
    "for j in range(0, 5):\n",
    "    param_list = [bandlength, cos_threshold, beta, delta, clustering_threshold]\n",
    "    random_grid = rd.sample(list(itertools.product(*param_list)), 10)\n",
    "\n",
    "    #random grid search with the different parameter vectors to obtain (close to) optimal parameter values based on the train set\n",
    "    for i in range(0, len(random_grid)):\n",
    "        print(i, \"bandlength:\" + str(random_grid[i][0]), \"cosine threshold:\" + str(random_grid[i][1]), \"beta:\" + str(random_grid[i][2]), \"delta:\" + str(random_grid[i][3]), \"clustering threshold:\"+str(random_grid[i][4]))\n",
    "        results = model_LSH(trainlist[j], titles, modelID_list, random_grid[i][0], random_grid[i][1], random_grid[i][2], random_grid[i][3], random_grid[i][4])\n",
    "        print(results) \n",
    "        fraccompare = results[0]\n",
    "        fraccompare2 = results[1]\n",
    "        f1 = results[2]\n",
    "        truepos = results[3]\n",
    "        falsepos = results[4]\n",
    "        falseneg = results[5]\n",
    "        trueneg = results[6]\n",
    "        pairqual = results[7]\n",
    "        paircompl = results[8]\n",
    "        \n",
    "        list_compare.append(fraccompare)\n",
    "        list_compare2.append(fraccompare2)\n",
    "#         list_f1star.append(f1star)\n",
    "        list_f1.append(f1)\n",
    "        list_truepos.append(truepos)\n",
    "        list_falsepos.append(falsepos)\n",
    "        list_falseneg.append(falseneg)\n",
    "        list_trueneg.append(trueneg)\n",
    "        list_pair_qual.append(pairqual)\n",
    "        list_pair_compl.append(paircompl)\n",
    "        \n",
    "    max_value = max(list_f1)\n",
    "    max_index = list_f1.index(max_value)\n",
    "    best_parameters = random_grid[max_index]\n",
    "    test_results = model_LSH(testlist[j], titles, modelID_list, best_parameters[0], best_parameters[1], best_parameters[2], best_parameters[3], best_parameters[4])\n",
    "    print(test_results)\n",
    "    fraccompare_test = test_results[0]\n",
    "    fraccompare2_test = test_results[1]\n",
    "#     f1star_test = test_results[2]\n",
    "    f1_test = test_results[2]\n",
    "    truepos_test = test_results[3]\n",
    "    falsepos_test = test_results[4]\n",
    "    falseneg_test = test_results[5]\n",
    "    trueneg_test = test_results[6]\n",
    "    pairqual_test = test_results[7]\n",
    "    paircompl_test = test_results[8]\n",
    "    \n",
    "    f1star_test = (2*pairqual_test*paircompl_test)/(pairqual_test+paircompl_test)\n",
    "    \n",
    "    \n",
    "    list_compare_test.append(fraccompare_test)\n",
    "    list_compare2_test.append(fraccompare2_test)\n",
    "    list_f1star_test.append(f1star_test)\n",
    "    list_f1_test.append(f1_test)\n",
    "    list_truepos_test.append(truepos_test)\n",
    "    list_falsepos_test.append(falsepos_test)\n",
    "    list_falseneg_test.append(falseneg_test)\n",
    "    list_trueneg_test.append(trueneg_test)\n",
    "    list_pair_qual_test.append(pairqual_test)\n",
    "    list_pair_compl_test.append(paircompl_test)\n",
    "    print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1839b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test_1_5 = model_LSH(testlist[4], titles, modelID_list, 1, 0.9, 0.3, 0.8, 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2083820e",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_result_1 = []\n",
    "total_result_1.append(result_test_1[2])\n",
    "total_result_1.append(result_test_1_2[2])\n",
    "total_result_1.append(result_test_1_3[2])\n",
    "total_result_1.append(result_test_1_4[2])\n",
    "total_result_1.append(result_test_1_5[2])\n",
    "total_result_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33656f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_f1_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c2f8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_compare2_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e0e1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_result_1_compare2 = []\n",
    "total_result_1_compare2.append(result_test_1[1])\n",
    "total_result_1_compare2.append(result_test_1_2[1])\n",
    "total_result_1_compare2.append(result_test_1_3[1])\n",
    "total_result_1_compare2.append(result_test_1_4[1])\n",
    "total_result_1_compare2.append(result_test_1_5[1])\n",
    "total_result_1_compare2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586abdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_compare_test_14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f994446c",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_f1_graph = []\n",
    "list_f1_graph.append(statistics.mean(list_f1_test_14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05135fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_compare2f1_graph = []\n",
    "list_compare2f1_graph.append(statistics.mean(list_compare2_test_14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2723e6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics.mean(list_compare2_test_14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263b7a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "list_f1star = []\n",
    "list_f1 = []\n",
    "list_compare = []\n",
    "list_compare2 = []\n",
    "list_pair_qual = []\n",
    "list_pair_compl= []\n",
    "list_truepos = []\n",
    "list_falsepos = []\n",
    "list_falseneg = []\n",
    "list_trueneg = []\n",
    "list_f1star_test = []\n",
    "list_f1_test = []\n",
    "list_compare_test = []\n",
    "list_compare2_test = []\n",
    "list_pair_qual_test = []\n",
    "list_pair_compl_test = []\n",
    "list_truepos_test = []\n",
    "list_falsepos_test = []\n",
    "list_falseneg_test = []\n",
    "list_trueneg_test = []\n",
    "list_f1_test_bandlength = []\n",
    "list_compare_test_bandlength = []\n",
    "list_compare2_test_bandlength = []\n",
    "\n",
    "#create random vectors for different parameter values\n",
    "bandlength =  [7,5,4,3,1]\n",
    "cos_threshold = [0.7, 0.8, 0.9]\n",
    "beta =  [0.2, 0.3, 0.4] \n",
    "delta = [0.7, 0.8, 0.9]\n",
    "clustering_threshold = [0.2, 0.15, 0.17, 0.22]\n",
    "\n",
    "for z in bandlength:\n",
    "    for j in range(0, 5):\n",
    "        param_list = [cos_threshold, beta, delta, clustering_threshold]\n",
    "        random_grid = rd.sample(list(itertools.product(*param_list)), 10)\n",
    "\n",
    "        #random grid search with the different parameter vectors to obtain (close to) optimal parameter values based on the train set\n",
    "        for i in range(0, len(random_grid)):\n",
    "            print(i, \"bandlength:\" + str(z), \"cosine threshold:\" + str(random_grid[i][0]), \"beta:\" + str(random_grid[i][1]), \"delta:\" + str(random_grid[i][2]), \"clustering threshold:\"+str(random_grid[i][3]))\n",
    "            results = model_LSH(trainlist[j], titles, modelID_list, z, random_grid[i][0], random_grid[i][1], random_grid[i][2], random_grid[i][3])\n",
    "            print(results) \n",
    "            fraccompare = results[0]\n",
    "            fraccompare2 = results[1]\n",
    "            f1 = results[2]\n",
    "            truepos = results[3]\n",
    "            falsepos = results[4]\n",
    "            falseneg = results[5]\n",
    "            trueneg = results[6]\n",
    "            pairqual = results[7]\n",
    "            paircompl = results[8]\n",
    "\n",
    "            list_compare.append(fraccompare)\n",
    "            list_compare2.append(fraccompare2)\n",
    "    #         list_f1star.append(f1star)\n",
    "            list_f1.append(f1)\n",
    "            list_truepos.append(truepos)\n",
    "            list_falsepos.append(falsepos)\n",
    "            list_falseneg.append(falseneg)\n",
    "            list_trueneg.append(trueneg)\n",
    "            list_pair_qual.append(pairqual)\n",
    "            list_pair_compl.append(paircompl)\n",
    "\n",
    "        max_value = max(list_f1)\n",
    "        max_index = list_f1.index(max_value)\n",
    "        best_parameters = random_grid[max_index]\n",
    "        test_results = model_LSH(testlist[j], titles, modelID_list, z, best_parameters[0], best_parameters[1], best_parameters[2], best_parameters[3])\n",
    "        print(test_results)\n",
    "        fraccompare_test = test_results[0]\n",
    "        fraccompare2_test = test_results[1]\n",
    "    #     f1star_test = test_results[2]\n",
    "        f1_test = test_results[2]\n",
    "        truepos_test = test_results[3]\n",
    "        falsepos_test = test_results[4]\n",
    "        falseneg_test = test_results[5]\n",
    "        trueneg_test = test_results[6]\n",
    "        pairqual_test = test_results[7]\n",
    "        paircompl_test = test_results[8]\n",
    "\n",
    "#         f1star_test = (2*pairqual_test*paircompl_test)/(pairqual_test+paircompl_test)\n",
    "\n",
    "        list_compare_test.append(fraccompare_test)\n",
    "        list_compare2_test.append(fraccompare2_test)\n",
    "#         list_f1star_test.append(f1star_test)\n",
    "        list_f1_test.append(f1_test)\n",
    "        list_truepos_test.append(truepos_test)\n",
    "        list_falsepos_test.append(falsepos_test)\n",
    "        list_falseneg_test.append(falseneg_test)\n",
    "        list_trueneg_test.append(trueneg_test)\n",
    "        list_pair_qual_test.append(pairqual_test)\n",
    "        list_pair_compl_test.append(paircompl_test)\n",
    "        print(j)\n",
    "        \n",
    "    list_f1_test_bandlength.append(list_f1_test)\n",
    "    list_compare_test_bandlength.append(list_compare_test)\n",
    "    list_compare2_test_bandlength.append(list_compare2_test)   \n",
    "    print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963e372d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_f1_test_7 = []\n",
    "list_f1_test_5 = []\n",
    "list_f1_test_4 = []\n",
    "list_f1_test_3 = []\n",
    "for i in range (0,3):\n",
    "    list_f1_test_7.append(list_f1_test_bandlength[0][i])\n",
    "    list_f1_test_5.append(list_f1_test_bandlength[0][i+3])\n",
    "    list_f1_test_4.append(list_f1_test_bandlength[0][i+6])\n",
    "    list_f1_test_3.append(list_f1_test_bandlength[0][i+9])\n",
    "list_f1_test_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef13b20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_f1_graph.append(statistics.mean(list_f1_test_7))\n",
    "list_f1_graph.append(statistics.mean(list_f1_test_5))\n",
    "list_f1_graph.append(statistics.mean(list_f1_test_4))\n",
    "list_f1_graph.append(statistics.mean(list_f1_test_3))\n",
    "list_f1_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3583c42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_f1_test_bandlength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3860db7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_compare2f1_test_7 = []\n",
    "list_compare2f1_test_5 = []\n",
    "list_compare2f1_test_4 = []\n",
    "list_compare2f1_test_3 = []\n",
    "for i in range (0,3):\n",
    "    list_compare2f1_test_7.append(list_compare2_test_bandlength[0][i])\n",
    "    list_compare2f1_test_5.append(list_compare2_test_bandlength[0][i+3])\n",
    "    list_compare2f1_test_4.append(list_compare2_test_bandlength[0][i+6])\n",
    "    list_compare2f1_test_3.append(list_compare2_test_bandlength[0][i+9])\n",
    "list_compare2f1_test_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c82bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_compare2f1_graph.append(statistics.mean(list_compare2f1_test_7))\n",
    "list_compare2f1_graph.append(statistics.mean(list_compare2f1_test_5))\n",
    "list_compare2f1_graph.append(statistics.mean(list_compare2f1_test_4))\n",
    "list_compare2f1_graph.append(statistics.mean(list_compare2f1_test_3))\n",
    "\n",
    "list_compare2f1_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9444b181",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_compare2f1_graph.append(mean_compare2_1)\n",
    "list_f1_graph.append(mean_f1_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13aace4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list_compare2f1_graph, list_f1_graph, 'k')\n",
    "plt.xlim(-0.01, 0.7)\n",
    "plt.ylim(0, 0.5)\n",
    "plt.xlabel(\"Fraction of Comparisons\")\n",
    "plt.ylabel(\"F1-measure\")\n",
    "plt.savefig('F1measure.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbb110a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_f1star_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7737f82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "allresults = []\n",
    "lsh_results = []\n",
    "lsh_results_1 = []\n",
    "list_bandlength = [14, 10, 7, 5, 4, 3, 1]\n",
    "for j in list_bandlength:\n",
    "    for i in range(0, bootstraps):\n",
    "        lsh_results = model_LSH_only(testlist[i], titles, modelID_list, j)\n",
    "        lsh_results_1.append(lsh_results)\n",
    "        print(i)\n",
    "    allresults.append(lsh_results_1)\n",
    "    lsh_results_1 = []\n",
    "    print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b1fcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_compare = 0\n",
    "sum_compare_list = []\n",
    "sum_compare2 = 0\n",
    "sum_compare2_list = []\n",
    "sum_f1star = 0\n",
    "sum_f1star_list = []\n",
    "sum_quality = 0\n",
    "sum_quality_list = []\n",
    "sum_compl = 0\n",
    "sum_compl_list = []\n",
    "for j in range(0,7):\n",
    "    for i in range(0,bootstraps):\n",
    "        sum_compare = sum_compare + allresults[j][i][0]\n",
    "        sum_compare2 = sum_compare2 + allresults[j][i][1]\n",
    "        sum_f1star = sum_f1star + allresults[j][i][2]\n",
    "        sum_quality = sum_quality + allresults[j][i][8]\n",
    "        sum_compl = sum_compl + allresults[j][i][9]\n",
    "    avg_compare = sum_compare / bootstraps\n",
    "    avg_compare2 = sum_compare2 / bootstraps\n",
    "    avg_f1star = sum_f1star / bootstraps\n",
    "    avg_quality = sum_quality / bootstraps\n",
    "    avg_compl = sum_compl / bootstraps\n",
    "    sum_compare_list.append(avg_compare)\n",
    "    sum_compare2_list.append(avg_compare2)\n",
    "    sum_f1star_list.append(avg_f1star)\n",
    "    sum_quality_list.append(avg_quality)\n",
    "    sum_compl_list.append(avg_compl)\n",
    "    sum_compare = 0\n",
    "    sum_compare2 = 0\n",
    "    sum_f1star = 0\n",
    "    sum_quality = 0\n",
    "    sum_compl = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f2bc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot f1* \n",
    "plt.plot(sum_compare2_list, sum_f1star_list, 'k')\n",
    "plt.xlim(-0.01, 1)\n",
    "plt.ylim(0, 0.35)\n",
    "plt.xlabel(\"Fraction of Comparisons\")\n",
    "plt.ylabel(\"F1*-measure\")\n",
    "plt.savefig('F1starmeasure.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55c8851",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot pair completeness\n",
    "plt.plot(sum_compare2_list, sum_compl_list, 'k')\n",
    "plt.xlim(-0.01, 1)\n",
    "plt.ylim(0, 1.03)\n",
    "plt.xlabel(\"Fraction of Comparisons\")\n",
    "plt.ylabel(\"Pair Completeness\")\n",
    "plt.savefig('Paircompleteness.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdbb409",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot pair quality\n",
    "plt.plot(sum_compare2_list, sum_quality_list, 'k')\n",
    "plt.xlim(-0.001, 0.2)\n",
    "plt.ylim(0, 0.3)\n",
    "plt.xlabel(\"Fraction of Comparisons\")\n",
    "plt.ylabel(\"Pair Quality\")\n",
    "plt.savefig('Pairquality.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9ce973",
   "metadata": {},
   "outputs": [],
   "source": [
    "brands = []\n",
    "for i in trainset:\n",
    "    if ('Brand' in new_data[i+1]['featuresMap']):\n",
    "        brands.append(new_data[i+1]['featuresMap']['Brand'])\n",
    "    elif ('Brand Name' in new_data[i+1]['featuresMap']):\n",
    "        brands.append(new_data[i+1]['featuresMap']['Brand Name'])\n",
    "    else:\n",
    "        brands.append('No result')\n",
    "for i in range(0,len(brands)):\n",
    "    brands[i] = brands[i].lower() \n",
    "brandslist = [re.sub(r'lg electronics', 'lg', item) for item in brands]\n",
    "brandslist2 = [re.sub(r'sceptre inc.', 'sceptre', item) for item in brandslist]\n",
    "brands_final = [re.sub(r'jvc tv', 'jvc', item) for item in brandslist2]\n",
    "brands_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51aa050",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedda56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_no_LSH(dataset, alltitles_list, modelID_list, cos_threshold, beta, delta, clustering_threshold):\n",
    "\n",
    "    modelID = []\n",
    "    for i in dataset:\n",
    "        modelID.append(modelID_list[i])\n",
    "    \n",
    "    titles_dataset = []\n",
    "    for i in dataset:\n",
    "        titles_dataset.append(alltitles_list[i])\n",
    "    \n",
    "    tvlist = [\"tv\" + str(i) for i in dataset]\n",
    "    \n",
    "    lengthone = (len(modelID),len(modelID))\n",
    "    sizeone = np.ones(lengthone)\n",
    "    dfones = pd.DataFrame(sizeone)\n",
    "    arronedata = dfones.to_numpy()\n",
    "    newarronedata = np.triu(arronedata)\n",
    "    dfnew5 = pd.DataFrame(newarronedata)\n",
    "\n",
    "    list_per_tv = []\n",
    "    for i in range(0, len(titles_dataset)):\n",
    "        list_per_tv.append(titles_dataset[i].split())\n",
    "\n",
    "    for i in range(0,len(list_per_tv)):\n",
    "        for j in range(0, len(list_per_tv[i])):\n",
    "            list_per_tv[i][j] = list_per_tv[i][j].lower()  \n",
    "\n",
    "    import re\n",
    "\n",
    "    pattern = re.compile(\"\\d+\")\n",
    "\n",
    "    for i in range(0,len(list_per_tv)):\n",
    "        for j in range(0, len(list_per_tv[i])-1):\n",
    "            if (list_per_tv[i][j+1] == 'inch' or list_per_tv[i][j+1] == '”' or list_per_tv[i][j+1] == '\"' or list_per_tv[i][j+1] == '-inch') and bool(pattern.match(list_per_tv[i][j])):\n",
    "                new_word = list_per_tv[i][j] + 'inch'\n",
    "                list_per_tv[i][j] = new_word\n",
    "            elif (list_per_tv[i][j+1] == 'hz' or list_per_tv[i][j+1] == 'hertz' or list_per_tv[i][j+1] == '-hz') and bool(pattern.match(list_per_tv[i][j])):\n",
    "                new_word2 = list_per_tv[i][j] + 'hz'\n",
    "                list_per_tv[i][j] = new_word2\n",
    "\n",
    "\n",
    "    filtered_per_tv = []            \n",
    "    for i in range(0,len(list_per_tv)): \n",
    "        new_list3 = [s.replace(\"(\", \"\") for s in list_per_tv[i]]\n",
    "        new_list4 = [s.replace(\"[\", \"\") for s in new_list3]\n",
    "        r = re.compile(\"philips|samsung|supersonic|sharp|nec|toshiba|\\\n",
    "        hisense|sony|lg|sanyo|coby|panasonic|sansui|vizio|viewsonic|sunbritetv|haier|\\\n",
    "        proscan|jvc|pyle|lg electronics|sceptre|magnavox|mitsubishi|compaq|hannspree|\\\n",
    "        sceptre inc.|upstar|seiki|rca|craig|affinity|jvc tv|naxa|westinghouse|epson|hp|\\\n",
    "        elo|pansonic|hello kitty|sigmac|[a-z0-9]*[0-9]+[^0-9,(\\s]*|[^0-9,(\\s]*[0-9]+[a-z0-9]*\")\n",
    "        new_list5 = list(filter(r.match, new_list4))\n",
    "        filtered_per_tv.append(new_list5)\n",
    "\n",
    "    filtered_per_tv_final = []    \n",
    "    for i in range(0,len(filtered_per_tv)):\n",
    "        new_list6 = [re.sub(r'\\\"|\\”|\\'', 'inch', item) for item in filtered_per_tv[i]]\n",
    "        new_list7 = [re.sub(r'-inch', 'inch', item) for item in new_list6]\n",
    "        new_list8 = [re.sub(r'-hz|hertz', 'hz', item) for item in new_list7]\n",
    "        new_list9 = [s.replace(\".0\", \"\") for s in new_list8]\n",
    "        new_list10 = [s.replace(\")\", \"\") for s in new_list9]\n",
    "    \n",
    "        filtered_per_tv_final.append(new_list10) \n",
    "\n",
    "    filtered_final = []\n",
    "    regex = re.compile(r'.{3,}inch')\n",
    "    for i in range(0, len(filtered_per_tv_final)):\n",
    "        filtered = [g for g in filtered_per_tv_final[i] if not regex.match(g)]\n",
    "        filtered_final.append(filtered)\n",
    "\n",
    "    shops = []\n",
    "    for i in dataset:\n",
    "         shops.append(new_data[i+1][\"shop\"])\n",
    "            \n",
    "    brands = []\n",
    "    for i in dataset:\n",
    "        if ('Brand' in new_data[i+1]['featuresMap']):\n",
    "            brands.append(new_data[i+1]['featuresMap']['Brand'])\n",
    "        elif ('Brand Name' in new_data[i+1]['featuresMap']):\n",
    "            brands.append(new_data[i+1]['featuresMap']['Brand Name'])\n",
    "        else:\n",
    "            brands.append('no result')\n",
    "            \n",
    "    for i in range(0,len(brands)):\n",
    "        brands[i] = brands[i].lower() \n",
    "    brandslist = [re.sub(r'lg electronics', 'lg', item) for item in brands]\n",
    "    brandslist2 = [re.sub(r'sceptre inc.', 'sceptre', item) for item in brandslist]\n",
    "    brands_final = [re.sub(r'jvc tv', 'jvc', item) for item in brandslist2]\n",
    "\n",
    "    tvlengthnew5 = (len(tvlist),len(tvlist))\n",
    "    Atvlengthnew5 = np.zeros(tvlengthnew5)\n",
    "    dftvlistnew5 = pd.DataFrame(Atvlengthnew5, index=tvlist, columns=tvlist)\n",
    "\n",
    "    for i in range(0, len(dataset)):\n",
    "        for j in range(0, len(dataset)):\n",
    "            if j > i:\n",
    "                if dfnew5.iloc[i,j] == 1:\n",
    "                    dftvlistnew5.iloc[i,j] = get_cosine_similarity(text_to_vector(titles_dataset[i]), text_to_vector(titles_dataset[j]))\n",
    "\n",
    "    for i in range(0, len(dataset)):\n",
    "        for j in range(0, len(dataset)):\n",
    "            if j > i and shops[i] == shops[j]:\n",
    "                dftvlistnew5.iloc[i,j] = -float('inf')\n",
    "            if j > i and brands_final[i] != brands_final[j] and brands_final[i] != 'no result' and brands_final[j] != 'no result':\n",
    "                dftvlistnew5.iloc[i,j] = -float('inf')\n",
    "\n",
    "    numofcomp = dftvlistnew5\n",
    "\n",
    "    numofcomp = numofcomp.replace(float('-inf'),0)\n",
    "    numberofcomparisons = sum(numofcomp[numofcomp > 0].count())\n",
    "\n",
    "    tvlength = (len(tvlist),len(tvlist))\n",
    "    Atvlength = np.zeros(tvlength)\n",
    "    dfsimlev = pd.DataFrame(Atvlength)\n",
    "\n",
    "    tvlength1 = (len(tvlist),len(tvlist))\n",
    "    Atvlength1 = np.zeros(tvlength1)\n",
    "    dfsimlevMW = pd.DataFrame(Atvlength1)\n",
    "    \n",
    "    list1=[]\n",
    "    regex = re.compile(r'[0-9]*')\n",
    "    for i in range(0, len(filtered_final)):\n",
    "        list2 = [re.sub(r'[0-9]*', '', item) for item in filtered_final[i]]\n",
    "        list1.append(list2)\n",
    "\n",
    "    list3=[]\n",
    "    regex = re.compile(r'[^0-9]*')\n",
    "    for i in range(0, len(filtered_final)):\n",
    "        list4 = [re.sub(r'[^0-9]*', '', item) for item in filtered_final[i]]\n",
    "        list3.append(list4)\n",
    "    #TMWM\n",
    "    sum_lev_MW = 0\n",
    "    sum_lev = 0\n",
    "    sum_length = 0\n",
    "    sum_length_allpairs = 0\n",
    "    for i in range(0, len(dftvlistnew5)):\n",
    "        for j in range(0, len(dftvlistnew5)):\n",
    "            if j > i and dfnew5.iloc[i,j] == 1 and dftvlistnew5.iloc[i,j] < cos_threshold and dftvlistnew5.iloc[i,j] >= 0:\n",
    "                for z in range(0, len(filtered_final[i])):\n",
    "                    for x in range(0, len(filtered_final[j])):\n",
    "                        first_model = list1[i][z]\n",
    "                        second_model = list1[j][x]\n",
    "                        text_distance = textdistance.levenshtein.normalized_distance(first_model, second_model)\n",
    "                        first_item = filtered_final[i][z]\n",
    "                        second_item = filtered_final[j][x]\n",
    "                        total_distance = textdistance.levenshtein.normalized_distance(first_item, second_item)\n",
    "                        length_allpairs = len(filtered_final[i][z])+len(filtered_final[j][x])\n",
    "                        sum_lev = sum_lev + (1-total_distance) * length_allpairs\n",
    "                        sum_length_allpairs = sum_length_allpairs + length_allpairs\n",
    "                        if text_distance < 0.1:\n",
    "                            first_word = list3[i][z]\n",
    "                            second_word = list3[j][x]\n",
    "                            numeric_distance = textdistance.levenshtein.normalized_distance(first_word, second_word)\n",
    "                            if numeric_distance > 0:\n",
    "                                dftvlistnew5.iloc[i,j] = -float('inf')\n",
    "                                length = len(filtered_final[i][z])+len(filtered_final[j][x])\n",
    "                                sum_length = sum_length + length\n",
    "                            else:\n",
    "                                first_string = filtered_final[i][z]\n",
    "                                second_string = filtered_final[j][x]\n",
    "                                string_distance = textdistance.levenshtein.normalized_distance(first_string, second_string)\n",
    "                                length = len(filtered_final[i][z])+len(filtered_final[j][x])\n",
    "                                sum_lev_MW = sum_lev_MW + (1-string_distance) * length\n",
    "                                sum_length = sum_length + length\n",
    "            if dftvlistnew5.iloc[i,j] != 0 and sum_length != 0:\n",
    "                avg_lev = sum_lev / sum_length_allpairs\n",
    "                avg_lev_MW = sum_lev_MW / sum_length\n",
    "                dfsimlev.iloc[i,j] = beta * dftvlistnew5.iloc[i,j] + (1-beta) * avg_lev\n",
    "                dfsimlevMW.iloc[i,j] = delta * avg_lev_MW + (1-delta) * dfsimlev.iloc[i,j]\n",
    "                sum_lev_MW = 0\n",
    "                sum_length = 0\n",
    "                sum_lev = 0\n",
    "                sum_length_allpairs = 0\n",
    "\n",
    "    for i in range(0, len(dfsimlevMW)):\n",
    "        for j in range(0, len(dfsimlevMW)):\n",
    "            if j > i and shops[i] == shops[j]:\n",
    "                dfsimlevMW.iloc[i,j] = -float('inf')\n",
    "            if j > i and brands_final[i] != brands_final[j] and brands_final[i] != 'no result' and brands_final[j] != 'no result':\n",
    "                dfsimlevMW.iloc[i,j] = -float('inf')\n",
    "\n",
    "\n",
    "\n",
    "    finalm = (len(dfsimlevMW),len(dfsimlevMW))\n",
    "    finalA = np.ones(finalm)\n",
    "    onematrix = pd.DataFrame(finalA)\n",
    "    dfsimlevMWone = onematrix.sub(dfsimlevMW)\n",
    "    finaldissimilaritymatrix = dfsimlevMWone.replace(1, 1000)\n",
    "    finaldissimilaritymatrix2 = finaldissimilaritymatrix.replace(float('inf'), 1000)\n",
    "    finaldissimilaritymatrix2numpy = finaldissimilaritymatrix2.to_numpy()\n",
    "    i_lower = np.tril_indices(len(tvlist), -1)\n",
    "    finaldissimilaritymatrix2numpy[i_lower] = finaldissimilaritymatrix2numpy.T[i_lower]\n",
    "    dffinaldissim = pd.DataFrame(finaldissimilaritymatrix2numpy)\n",
    "\n",
    "    clustering = AgglomerativeClustering(n_clusters = None, affinity='precomputed', linkage='complete', distance_threshold=clustering_threshold, compute_full_tree= True).fit(dffinaldissim)\n",
    "    labelsclust = clustering.labels_\n",
    "\n",
    "    originalm = (len(modelID),len(modelID))\n",
    "    originalA = np.zeros(originalm)\n",
    "    originalcorrect = pd.DataFrame(originalA, index=modelID, columns=modelID)\n",
    "    for i in range(0, len(modelID)):\n",
    "        for j in range(0, len(modelID)):\n",
    "            if modelID[i] == modelID[j]:\n",
    "                originalcorrect.iloc[i,j] = 1\n",
    "    originalcorrect.values[[np.arange(originalcorrect.shape[0])]*2] = 0\n",
    "\n",
    "    clusterdm = (len(modelID),len(modelID))\n",
    "    clusteredA = np.zeros(clusterdm)\n",
    "    clusteredmat = pd.DataFrame(clusteredA, index=modelID, columns=modelID)\n",
    "    check = pd.DataFrame(list(zip(labelsclust, modelID)))\n",
    "    for i in range(0, len(modelID)):\n",
    "        for j in range(0, len(modelID)):\n",
    "            if check[0][i] == check[0][j]:\n",
    "                clusteredmat.iloc[i,j] = 1\n",
    "    clusteredmat.values[[np.arange(clusteredmat.shape[0])]*2] = 0\n",
    "\n",
    "    falsenegatives = 0\n",
    "    falsepositives = 0\n",
    "    truenegatives = 0\n",
    "    truepositives = 0\n",
    "\n",
    "    for i in range(0, len(originalcorrect)):\n",
    "        for j in range(0, len(originalcorrect)):\n",
    "            if j > i:\n",
    "                if  clusteredmat.iloc[i,j] == 0 and originalcorrect.iloc[i,j] == 0:\n",
    "                    truenegatives = truenegatives + 1\n",
    "                elif  clusteredmat.iloc[i,j] == 0 and originalcorrect.iloc[i,j] == 1:\n",
    "                    falsenegatives = falsenegatives + 1\n",
    "                elif  clusteredmat.iloc[i,j] == 1 and originalcorrect.iloc[i,j] == 0:\n",
    "                    falsepositives = falsepositives + 1\n",
    "                elif  clusteredmat.iloc[i,j] == 1 and originalcorrect.iloc[i,j] == 1:\n",
    "                    truepositives = truepositives + 1\n",
    "\n",
    "#     print(\"truepositives = \"+ str(truepositives))\n",
    "#     print(\"truenegatives = \" + str(truenegatives))\n",
    "#     print(\"falsepositives = \" + str(falsepositives))\n",
    "#     print(\"falsenegatives = \"+ str(falsenegatives))\n",
    "    f1measure = (truepositives/(truepositives+0.5*(falsenegatives+falsepositives)))\n",
    "#     print(\"f1measure = \" + str(f1measure))\n",
    "\n",
    "    pairquality = (truepositives)/(numberofcomparisons) \n",
    "    paircompleteness = (truepositives)/(truepositives + falsenegatives)\n",
    "#     print(\"Pair quality = \", str(pairquality))\n",
    "#     print(\"Pair completeness = \", str(paircompleteness))\n",
    "    f1starmeasure = (2*pairquality*paircompleteness)/(pairquality+paircompleteness)\n",
    "#     print(\"f1starmeasure = \" + str(f1starmeasure))\n",
    "    \n",
    "    return (fracofcomparisons, fracofcomparisons2, f1starmeasure, f1measure, truepositives, falsepositives, falsenegatives, truenegatives, pairquality, paircompleteness) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e39d5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
